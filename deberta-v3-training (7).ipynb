{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":163088908,"sourceType":"kernelVersion"}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install seqeval evaluate -q","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.simplefilter('ignore')\n\nfrom itertools import chain\nfrom pathlib import Path\nimport json\nimport torch\nfrom transformers import Trainer, TrainingArguments, AutoTokenizer\nfrom transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\nimport evaluate\nfrom datasets import Dataset, features\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom functools import partial\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAINING_MODEL_PATH = \"microsoft/deberta-v3-base\"\nTRAINING_MAX_LENGTH = 2048\nEVAL_MAX_LENGTH = 3072\nCONF_THRESH = 0.9\nLR = 5e-4  # Note: lr for LoRA should be order of magnitude larger than usual fine-tuning\nLR_SCHEDULER_TYPE = \"linear\"\nNUM_EPOCHS = 4\nBATCH_SIZE = 8\nEVAL_BATCH_SIZE = 8\nGRAD_ACCUMULATION_STEPS = 16 // BATCH_SIZE\nWARMUP_RATIO = 0.1\nWEIGHT_DECAY = 0.01\nFREEZE_EMBEDDING = False\nFREEZE_LAYERS = 6\nLORA_R = 16  # rank of the A and B matricies, the lower the more efficient but more approximate\nLORA_ALPHA = LORA_R * 2  # alpha/r is multiplied to BA\nAMP = True\nN_SPLITS = 4\nNEGATIVE_RATIO = 0.3  # downsample ratio of negative samples in the training set\nOUTPUT_DIR = \"output\"\nPath(OUTPUT_DIR).mkdir(exist_ok=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.simplefilter('ignore')\n\nfrom itertools import chain\nfrom pathlib import Path\nimport json\nimport torch\nfrom transformers import Trainer, TrainingArguments, AutoTokenizer\nfrom transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\nimport evaluate\nfrom datasets import Dataset, features\n\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from functools import partial","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ****DATA SELECTION and LABEL MAPPING****","metadata":{}},{"cell_type":"code","source":"data = json.load(open(\"/kaggle/input/pii-detection-removal-from-educational-data/train.json\"))\next_data = json.load(open(\"/kaggle/input/fix-punctuation-tokenization-external-dataset/pii_dataset_fixed.json\"))\next_more = json.load(open(\"/kaggle/input/fix-punctuation-tokenization-external-dataset/moredata_dataset_fixed.json\"))\n# data[3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p=[]\nn=[]\nfor d in data:\n    if any(np.array(d[\"labels\"]) != 'O'): p.append(d)\n    else: n.append(d) \n\nprint(\"The length of data: \", len(data))  \nprint(\"The number of positive samples is: \", len(p))\nprint(\"The number of negative samples is: \", len(n))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pex=[]\nnex=[]\nfor d in ext_data:\n    if any(np.array(d[\"labels\"]) != 'O'): pex.append(d)\n    else: nex.append(d) \n\nprint(\"The length of data: \", len(ext_data))  \nprint(\"The number of positive samples is: \", len(pex))\nprint(\"The number of negative samples is: \", len(nex))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pexm=[]\nnexm=[]\nfor d in ext_more:\n    if any(np.array(d[\"labels\"]) != 'O'): pexm.append(d)\n    else: nexm.append(d) \n\nprint(\"The length of data: \", len(ext_more))  \nprint(\"The number of positive samples is: \", len(pexm))\nprint(\"The number of negative samples is: \", len(nexm))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Total size of the data: \", len(data+ext_more+ext_data))\nprint(\"Out of which data with no positive label: \", len(n+nex+nexm))\nprint(\"Paragraphs with personal info: \", len(p+pex+pexm))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data + ext_data + ext_more\nlen(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\nlabel2id = {l: i for i,l in enumerate(all_labels)}\nid2label = {v:k for k,v in label2id.items()}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ****TOKENIZER FUNCTION****\n\n","metadata":{}},{"cell_type":"code","source":"def tokenize(example, tokenizer, label2id, max_length):\n\n    # rebuild text from tokens\n    text = []\n    labels = []\n\n    for t, l, ws in zip(\n        example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]\n    ):\n        text.append(t)\n        labels.extend([l] * len(t))\n\n        if ws:\n            text.append(\" \")\n            labels.append(\"O\")\n\n    # actual tokenization\n    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, max_length=max_length)\n\n    labels = np.array(labels)\n\n    text = \"\".join(text)\n    token_labels = []\n\n    for start_idx, end_idx in tokenized.offset_mapping:\n        # CLS token\n        if start_idx == 0 and end_idx == 0:\n            token_labels.append(label2id[\"O\"])\n            continue\n\n        # case when token starts with whitespace\n        if text[start_idx].isspace():\n            start_idx += 1\n\n        token_labels.append(label2id[labels[start_idx]])\n\n    length = len(tokenized.input_ids)\n\n    return {**tokenized, \"labels\": token_labels, \"length\": length}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAINING_MODEL_PATH = \"microsoft/deberta-v3-base\"\nTRAINING_MAX_LENGTH = 3096\nOUTPUT_DIR = \"output\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(TRAINING_MODEL_PATH)\n\nds = Dataset.from_dict({\n    \"full_text\": [x[\"full_text\"] for x in data],\n    \"document\": [str(x[\"document\"]) for x in data],\n    \"tokens\": [x[\"tokens\"] for x in data],\n    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n    \"provided_labels\": [x[\"labels\"] for x in data],\n    \"num_tokens\": [len(x[\"tokens\"]) for x in data],\n    \"len_label\": [int(len(x[\"tokens\"])/200) for x in data]\n\n})\nds = ds.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id, \"max_length\": TRAINING_MAX_LENGTH}, num_proc=3)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count = 0\nfor c in ds[\"len_label\"]:\n    if c > 8:\n        count+=1\n        \nprint(count)       \nds = ds.filter(lambda x: x[\"len_label\"]<=8)        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ****COMPUTE METRICS****","metadata":{}},{"cell_type":"code","source":"from seqeval.metrics import recall_score, precision_score\nfrom seqeval.metrics import classification_report\nfrom seqeval.metrics import f1_score\n\ndef compute_metrics(p, all_labels):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    # Remove ignored index (special tokens)\n    true_predictions = [\n        [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    \n    recall = recall_score(true_labels, true_predictions)\n    precision = precision_score(true_labels, true_predictions)\n    f1_score = (1 + 5*5) * recall * precision / (5*5*precision + recall)\n    \n    results = {\n        'recall': recall,\n        'precision': precision,\n        'f1': f1_score\n    }\n    return results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 8","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = TrainingArguments(\n    output_dir=OUTPUT_DIR, \n    fp16=True,\n    learning_rate=2e-5,\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=8 // batch_size,\n    report_to=\"none\",\n    per_device_eval_batch_size = 4,\n    evaluation_strategy=\"steps\",\n    eval_steps=50,\n    eval_delay=100,\n    save_strategy=\"steps\",\n    save_steps=50,\n    save_total_limit=1,\n    logging_steps=10,\n    do_eval=True,\n    lr_scheduler_type='cosine',\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    warmup_ratio=0.1,\n    weight_decay=0.01,\n#     metric_for_best_model \n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# args = TrainingArguments(\n#     output_dir=OUTPUT_DIR,\n#     fp16=AMP,\n#     learning_rate=LR,\n#     num_train_epochs=4,\n#     per_device_train_batch_size=BATCH_SIZE,\n#     per_device_eval_batch_size=EVAL_BATCH_SIZE,\n#     gradient_accumulation_steps=GRAD_ACCUMULATION_STEPS,\n#     report_to=\"none\",\n#     evaluation_strategy=\"steps\",\n#     eval_steps=50,\n#     eval_delay=100,\n#     save_strategy=\"steps\",\n#     save_steps=50,\n#     save_total_limit=1,\n#     logging_steps=10,\n#     metric_for_best_model=\"f1\",\n#     greater_is_better=True,\n#     load_best_model_at_end=True,\n#     overwrite_output_dir=True,\n#     lr_scheduler_type=LR_SCHEDULER_TYPE,\n#     warmup_ratio=WARMUP_RATIO,\n#     weight_decay=WEIGHT_DECAY,\n# )\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=5)\nfold_idx = 1\n\n# split = []\nsplits = skf.split(ds, ds[\"len_label\"])\nfor train_index, test_index in skf.split(ds, ds[\"len_label\"]):\n    # Create train and test datasets for this split\n    train_ds = ds.select(train_index)\n    test_ds = ds.select(test_index)\n    \n    \n    model = AutoModelForTokenClassification.from_pretrained(\n    TRAINING_MODEL_PATH,\n    num_labels=len(all_labels),\n    id2label=id2label,\n    label2id=label2id,\n    ignore_mismatched_sizes=True\n    )\n    \n    collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n    \n    \n    trainer = Trainer(\n    model=model, \n    args=args, \n    eval_dataset = test_ds,    \n    train_dataset= train_ds,\n    data_collator=collator, \n    tokenizer=tokenizer,\n    compute_metrics=partial(compute_metrics, all_labels=all_labels),\n)\n    \n#     %%time\n    trainer.train()\n    eval_res = trainer.evaluate(eval_dataset=test_ds)\n    with open(os.path.join(args.output_dir, \"eval_result.json\"), \"w\") as f:\n        json.dump(eval_res, f)\n    trainer.model = trainer.model.base_model.merge_and_unload()\n    trainer.save_model(os.path.join(OUTPUT_DIR, f\"fold_{fold_idx}\", \"best\"))    \n    del trainer\n    gc.collect()\n    torch.cuda.empty_cache()\n    fold_idx += 1\n    \n#     split.append((train_ds, test_ds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainer.save_model(\"deberta3base\")\n# tokenizer.save_pretrained(\"deberta3base\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}