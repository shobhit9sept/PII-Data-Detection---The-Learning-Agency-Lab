{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":7784594,"sourceType":"datasetVersion","datasetId":4555967},{"sourceId":169885743,"sourceType":"kernelVersion"}],"dockerImageVersionId":30636,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### 🚀 Single deberta model can achieve 0.966 on LB\n#### Here's how I trained the model\n* freeze first 6 layers but made embedding layer trainable\n* used `MAX_TRAINING_LENGTH = 3072` (no overflowing)\n* splitted according to `document_id % 4`. All folds achieved >0.960 on LB but fold-2 achieved the best result\n* added MPWARE's [dataset](https://www.kaggle.com/datasets/mpware/pii-mixtral8x7b-generated-essays?select=mpware_mixtral8x7b_v1.1-no-i-username.json) (no `I-USERNAME` version) to the training set of each fold\n* hyperparameters:\n  - lr: 2.5e-5\n  - lr_scheduler: linear\n  - epoch: 3\n  - effective_batch_size: 16\n  - warmup_ratio: 0.1\n  - weight_decay: 0.01\n  - AMP: True","metadata":{}},{"cell_type":"code","source":"import json\nimport os\nimport re\nimport bisect\nfrom pathlib import Path\n\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom datasets import Dataset\nfrom spacy.lang.en import English\nfrom transformers.models.deberta_v2 import DebertaV2ForTokenClassification, DebertaV2TokenizerFast\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase\nfrom transformers.trainer import Trainer\nfrom transformers.training_args import TrainingArguments\nfrom transformers.data.data_collator import DataCollatorForTokenClassification","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-04-16T14:08:03.027922Z","iopub.execute_input":"2024-04-16T14:08:03.028427Z","iopub.status.idle":"2024-04-16T14:08:27.744815Z","shell.execute_reply.started":"2024-04-16T14:08:03.028383Z","shell.execute_reply":"2024-04-16T14:08:27.743450Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Config & Parameters","metadata":{}},{"cell_type":"code","source":"INFERENCE_MAX_LENGTH = 3500\nCONF_THRESH = 0.90  # threshold for \"O\" class\nURL_THRESH = 0.1  # threshold for URL\nAMP = True\n# MODEL_PATH = '/kaggle/input/37vp4pjt'\nMODEL_PATH = '/kaggle/input/deberta-v3-training-large-peft/deberta3base'\nDATA_DIR = '/kaggle/input/pii-detection-removal-from-educational-data/'","metadata":{"execution":{"iopub.status.busy":"2024-04-16T14:24:51.234866Z","iopub.execute_input":"2024-04-16T14:24:51.235415Z","iopub.status.idle":"2024-04-16T14:24:51.242593Z","shell.execute_reply.started":"2024-04-16T14:24:51.235377Z","shell.execute_reply":"2024-04-16T14:24:51.240754Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"nlp = English() \n\ndef find_span(target: list[str], document: list[str]) -> list[list[int]]:\n    idx = 0\n    spans = []\n    span = []\n\n    for i, token in enumerate(document):\n        if token != target[idx]:\n            idx = 0\n            span = []\n            continue\n        span.append(i)\n        idx += 1\n        if idx == len(target):\n            spans.append(span)\n            span = []\n            idx = 0\n            continue\n    \n    return spans","metadata":{"execution":{"iopub.status.busy":"2024-04-16T14:08:27.764095Z","iopub.execute_input":"2024-04-16T14:08:27.765272Z","iopub.status.idle":"2024-04-16T14:08:28.158888Z","shell.execute_reply.started":"2024-04-16T14:08:27.765173Z","shell.execute_reply":"2024-04-16T14:08:28.157871Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def spacy_to_hf(data: dict, idx: int) -> slice:\n    \"\"\"\n    Given an index of spacy token, return corresponding indices in deberta's output.\n    We use this to find indice of URL tokens later.\n    \"\"\"\n    str_range = np.where(np.array(data[\"token_map\"]) == idx)[0]\n    start_idx = bisect.bisect_left([off[1] for off in data[\"offset_mapping\"]], str_range.min())\n    end_idx = start_idx\n    while end_idx < len(data[\"offset_mapping\"]):\n        if str_range.max() > data[\"offset_mapping\"][end_idx][1]:\n            end_idx += 1\n            continue\n        break\n    token_range = slice(start_idx, end_idx+1)\n    return token_range","metadata":{"execution":{"iopub.status.busy":"2024-04-16T14:08:28.160394Z","iopub.execute_input":"2024-04-16T14:08:28.160759Z","iopub.status.idle":"2024-04-16T14:08:28.169961Z","shell.execute_reply.started":"2024-04-16T14:08:28.160729Z","shell.execute_reply":"2024-04-16T14:08:28.168503Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizer","metadata":{}},{"cell_type":"code","source":"class CustomTokenizer:\n    def __init__(self, tokenizer: PreTrainedTokenizerBase, max_length: int) -> None:\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __call__(self, example: dict) -> dict:\n        text = []\n        token_map = []\n\n        for idx, (t, ws) in enumerate(zip(example[\"tokens\"], example[\"trailing_whitespace\"])):\n            text.append(t)\n            token_map.extend([idx]*len(t))\n            if ws:\n                text.append(\" \")\n                token_map.append(-1)\n\n        tokenized = self.tokenizer(\n            \"\".join(text),\n            return_offsets_mapping=True,\n            truncation=True,\n            max_length=self.max_length,\n        )\n\n        return {**tokenized,\"token_map\": token_map,}","metadata":{"execution":{"iopub.status.busy":"2024-04-16T14:08:28.173838Z","iopub.execute_input":"2024-04-16T14:08:28.174819Z","iopub.status.idle":"2024-04-16T14:08:28.184263Z","shell.execute_reply.started":"2024-04-16T14:08:28.174783Z","shell.execute_reply":"2024-04-16T14:08:28.183122Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"with open(str(Path(DATA_DIR).joinpath(\"test.json\")), \"r\") as f:\n    data = json.load(f)\n\nds = Dataset.from_dict({\n    \"full_text\": [x[\"full_text\"] for x in data],\n    \"document\": [x[\"document\"] for x in data],\n    \"tokens\": [x[\"tokens\"] for x in data],\n    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n})\n\ntokenizer = DebertaV2TokenizerFast.from_pretrained(MODEL_PATH)\nds = ds.map(CustomTokenizer(tokenizer=tokenizer, max_length=INFERENCE_MAX_LENGTH), num_proc=os.cpu_count())","metadata":{"execution":{"iopub.status.busy":"2024-04-16T14:24:55.468822Z","iopub.execute_input":"2024-04-16T14:24:55.469329Z","iopub.status.idle":"2024-04-16T14:24:55.697468Z","shell.execute_reply.started":"2024-04-16T14:24:55.469285Z","shell.execute_reply":"2024-04-16T14:24:55.695337Z"},"trusted":true},"execution_count":15,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:389\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 389\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:110\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 110\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:158\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n","\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/kaggle/input/deberta-v3-training-large-peft/deberta3base'. Use `repo_type` argument if needed.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 11\u001b[0m\n\u001b[1;32m      2\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      4\u001b[0m ds \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_dict({\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_text\u001b[39m\u001b[38;5;124m\"\u001b[39m: [x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data],\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m: [x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data],\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: [x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data],\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrailing_whitespace\u001b[39m\u001b[38;5;124m\"\u001b[39m: [x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrailing_whitespace\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data],\n\u001b[1;32m      9\u001b[0m })\n\u001b[0;32m---> 11\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mDebertaV2TokenizerFast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m ds \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mmap(CustomTokenizer(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, max_length\u001b[38;5;241m=\u001b[39mINFERENCE_MAX_LENGTH), num_proc\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mcpu_count())\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1951\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer_file\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m vocab_files:\n\u001b[1;32m   1949\u001b[0m     \u001b[38;5;66;03m# Try to get the tokenizer config to see if there are versioned tokenizer files.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m     fast_tokenizer_file \u001b[38;5;241m=\u001b[39m FULL_TOKENIZER_FILE\n\u001b[0;32m-> 1951\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1961\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1962\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1963\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1965\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1966\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1967\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m   1968\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:454\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    455\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    456\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n","\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: '/kaggle/input/deberta-v3-training-large-peft/deberta3base'. Please provide either the path to a local folder or the repo_id of a model on the Hub."],"ename":"OSError","evalue":"Incorrect path_or_model_id: '/kaggle/input/deberta-v3-training-large-peft/deberta3base'. Please provide either the path to a local folder or the repo_id of a model on the Hub.","output_type":"error"}]},{"cell_type":"markdown","source":"# Instantiate the Trainer","metadata":{}},{"cell_type":"code","source":"model = DebertaV2ForTokenClassification.from_pretrained(MODEL_PATH)\ncollator = DataCollatorForTokenClassification(tokenizer)\nargs = TrainingArguments(\".\", per_device_eval_batch_size=4, report_to=\"none\", fp16=False)\ntrainer = Trainer(\n    model=model, args=args, data_collator=collator, tokenizer=tokenizer,\n)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-16T14:08:30.355288Z","iopub.status.idle":"2024-04-16T14:08:30.355751Z","shell.execute_reply.started":"2024-04-16T14:08:30.355527Z","shell.execute_reply":"2024-04-16T14:08:30.355548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"predictions = trainer.predict(ds).predictions  # (n_sample, len, n_labels)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T14:08:30.358208Z","iopub.status.idle":"2024-04-16T14:08:30.358852Z","shell.execute_reply.started":"2024-04-16T14:08:30.358616Z","shell.execute_reply":"2024-04-16T14:08:30.358641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ds[\"\"]","metadata":{"execution":{"iopub.status.busy":"2024-04-16T14:08:30.361229Z","iopub.status.idle":"2024-04-16T14:08:30.361961Z","shell.execute_reply.started":"2024-04-16T14:08:30.361750Z","shell.execute_reply":"2024-04-16T14:08:30.361772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Post-processing","metadata":{}},{"cell_type":"code","source":"pred_softmax = torch.softmax(torch.from_numpy(predictions), dim=2).numpy()\nid2label = model.config.id2label\no_index = model.config.label2id[\"O\"]\npreds = predictions.argmax(-1)\npreds_without_o = pred_softmax.copy()\npreds_without_o[:,:,o_index] = 0\npreds_without_o = preds_without_o.argmax(-1)\no_preds = pred_softmax[:,:,o_index]\npreds_final = np.where(o_preds < CONF_THRESH, preds_without_o , preds)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T14:08:30.363631Z","iopub.status.idle":"2024-04-16T14:08:30.364070Z","shell.execute_reply.started":"2024-04-16T14:08:30.363864Z","shell.execute_reply":"2024-04-16T14:08:30.363885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"pred = 10*1924*13","metadata":{}},{"cell_type":"code","source":"len(preds_final[0])","metadata":{"execution":{"iopub.status.busy":"2024-04-16T14:08:30.365963Z","iopub.status.idle":"2024-04-16T14:08:30.366441Z","shell.execute_reply.started":"2024-04-16T14:08:30.366181Z","shell.execute_reply":"2024-04-16T14:08:30.366200Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds[\"token_map\"][0]","metadata":{"execution":{"iopub.status.busy":"2024-04-16T14:08:30.368594Z","iopub.status.idle":"2024-04-16T14:08:30.369046Z","shell.execute_reply.started":"2024-04-16T14:08:30.368823Z","shell.execute_reply":"2024-04-16T14:08:30.368842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_final","metadata":{"execution":{"iopub.status.busy":"2024-04-16T14:08:30.371535Z","iopub.status.idle":"2024-04-16T14:08:30.371954Z","shell.execute_reply.started":"2024-04-16T14:08:30.371759Z","shell.execute_reply":"2024-04-16T14:08:30.371777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed =[]\npairs = set()\n\n# Iterate over document\nfor p, token_map, offsets, tokens, doc in zip(\n    preds_final, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"]\n):\n    # Iterate over sequence\n    for token_pred, (start_idx, end_idx) in zip(p, offsets):\n        label_pred = id2label[token_pred]\n\n        if start_idx + end_idx == 0:\n            # [CLS] token i.e. BOS\n            continue\n\n        if token_map[start_idx] == -1:\n            start_idx += 1\n\n        # ignore \"\\n\\n\"\n        while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n            start_idx += 1\n\n        if start_idx >= len(token_map): \n            break\n\n        token_id = token_map[start_idx]\n        pair = (doc, token_id)\n\n        # ignore certain labels and whitespace\n        if label_pred in (\"O\", \"B-EMAIL\", \"B-URL_PERSONAL\", \"B-PHONE_NUM\", \"I-PHONE_NUM\") or token_id == -1:\n            continue        \n\n        if pair in pairs:\n            continue\n            \n        processed.append(\n            {\"document\": doc, \"token\": token_id, \"label\": label_pred, \"token_str\": tokens[token_id]}\n        )\n        pairs.add(pair)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T14:08:30.373495Z","iopub.status.idle":"2024-04-16T14:08:30.373890Z","shell.execute_reply.started":"2024-04-16T14:08:30.373696Z","shell.execute_reply":"2024-04-16T14:08:30.373715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed","metadata":{"execution":{"iopub.status.busy":"2024-04-16T14:08:30.375409Z","iopub.status.idle":"2024-04-16T14:08:30.375821Z","shell.execute_reply.started":"2024-04-16T14:08:30.375620Z","shell.execute_reply":"2024-04-16T14:08:30.375639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## URL\nWe use spacy tokenizer's url match on each token to find all URL_PERSONAL candidates first.<br>\nThen compare the deberta's probability over the detected URL span againt the threshold.","metadata":{}},{"cell_type":"code","source":"url_whitelist = [\n    \"wikipedia.org\",\n    \"coursera.org\",\n    \"google.com\",\n    \".gov\",\n]\nurl_whitelist_regex = re.compile(\"|\".join(url_whitelist))\n\nfor row_idx, _data in enumerate(ds):\n    for token_idx, token in enumerate(_data[\"tokens\"]):\n        if not nlp.tokenizer.url_match(token):\n            continue\n        print(f\"Found URL: {token}\")\n        if url_whitelist_regex.search(token) is not None:\n            print(\"The above is in the whitelist\")\n            continue\n        input_idxs = spacy_to_hf(_data, token_idx)\n        probs = pred_softmax[row_idx, input_idxs, model.config.label2id[\"B-URL_PERSONAL\"]]\n        if probs.mean() > URL_THRESH:\n            print(\"The above is PII\")\n            processed.append(\n                {\n                    \"document\": _data[\"document\"], \n                    \"token\": token_idx, \n                    \"label\": \"B-URL_PERSONAL\", \n                    \"token_str\": token\n                }\n            )\n            pairs.add((_data[\"document\"], token_idx))\n        else:\n            print(\"The above is not PII\")","metadata":{"execution":{"iopub.status.busy":"2024-04-16T14:08:30.376950Z","iopub.status.idle":"2024-04-16T14:08:30.377346Z","shell.execute_reply.started":"2024-04-16T14:08:30.377138Z","shell.execute_reply":"2024-04-16T14:08:30.377156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Email & Phone number","metadata":{}},{"cell_type":"code","source":"email_regex = re.compile(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+')\nphone_num_regex = re.compile(r\"(\\(\\d{3}\\)\\d{3}\\-\\d{4}\\w*|\\d{3}\\.\\d{3}\\.\\d{4})\\s\")\nemails = []\nphone_nums = []\n\nfor _data in ds:\n    # email\n    for token_idx, token in enumerate(_data[\"tokens\"]):\n        if re.fullmatch(email_regex, token) is not None:\n            emails.append(\n                {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": \"B-EMAIL\", \"token_str\": token}\n            )\n    # phone number\n    matches = phone_num_regex.findall(_data[\"full_text\"])\n    if not matches:\n        continue\n    for match in matches:\n        target = [t.text for t in nlp.tokenizer(match)]\n        matched_spans = find_span(target, _data[\"tokens\"])\n    for matched_span in matched_spans:\n        for intermediate, token_idx in enumerate(matched_span):\n            prefix = \"I\" if intermediate else \"B\"\n            phone_nums.append(\n                {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": f\"{prefix}-PHONE_NUM\", \"token_str\": _data[\"tokens\"][token_idx]}\n            )","metadata":{"execution":{"iopub.status.busy":"2024-04-16T14:08:30.378574Z","iopub.status.idle":"2024-04-16T14:08:30.378956Z","shell.execute_reply.started":"2024-04-16T14:08:30.378767Z","shell.execute_reply":"2024-04-16T14:08:30.378786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 🤝 Submission hand-in","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame(processed + emails + phone_nums)\ndf[\"row_id\"] = list(range(len(df)))\ndf.head(100)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T14:08:30.380622Z","iopub.status.idle":"2024-04-16T14:08:30.381023Z","shell.execute_reply.started":"2024-04-16T14:08:30.380820Z","shell.execute_reply":"2024-04-16T14:08:30.380838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[[\"row_id\", \"document\", \"token\", \"label\"]].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T14:08:30.382327Z","iopub.status.idle":"2024-04-16T14:08:30.382722Z","shell.execute_reply.started":"2024-04-16T14:08:30.382531Z","shell.execute_reply":"2024-04-16T14:08:30.382549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}