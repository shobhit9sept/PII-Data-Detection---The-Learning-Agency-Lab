{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":7784594,"sourceType":"datasetVersion","datasetId":4555967},{"sourceId":7803679,"sourceType":"datasetVersion","datasetId":4340749},{"sourceId":7908837,"sourceType":"datasetVersion","datasetId":4646023}],"dockerImageVersionId":30683,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Part 1: Environment Setup and Data Loading\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport json\nimport argparse\nfrom itertools import chain\nfrom pathlib import Path\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\nfrom datasets import Dataset\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n #   for filename in filenames:\n  #      print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","metadata":{"execution":{"iopub.status.busy":"2024-04-20T12:28:14.549467Z","iopub.execute_input":"2024-04-20T12:28:14.550142Z","iopub.status.idle":"2024-04-20T12:28:33.593992Z","shell.execute_reply.started":"2024-04-20T12:28:14.550110Z","shell.execute_reply":"2024-04-20T12:28:33.593192Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-04-20 12:28:25.147937: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-20 12:28:25.148042: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-20 12:28:25.276888: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"train_path = \"/kaggle/input/pii-detection-removal-from-educational-data/train.json\"\ndf = pd.read_json(train_path)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-20T12:28:33.595531Z","iopub.execute_input":"2024-04-20T12:28:33.596083Z","iopub.status.idle":"2024-04-20T12:28:36.018875Z","shell.execute_reply.started":"2024-04-20T12:28:33.596058Z","shell.execute_reply":"2024-04-20T12:28:36.017905Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   document                                          full_text  \\\n0         7  Design Thinking for innovation reflexion-Avril...   \n1        10  Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...   \n2        16  Reporting process\\n\\nby Gilberto Gamboa\\n\\nCha...   \n3        20  Design Thinking for Innovation\\n\\nSindy Samaca...   \n4        56  Assignment:  Visualization Reflection  Submitt...   \n\n                                              tokens  \\\n0  [Design, Thinking, for, innovation, reflexion,...   \n1  [Diego, Estrada, \\n\\n, Design, Thinking, Assig...   \n2  [Reporting, process, \\n\\n, by, Gilberto, Gambo...   \n3  [Design, Thinking, for, Innovation, \\n\\n, Sind...   \n4  [Assignment, :,   , Visualization,  , Reflecti...   \n\n                                 trailing_whitespace  \\\n0  [True, True, True, True, False, False, True, F...   \n1  [True, False, False, True, True, False, False,...   \n2  [True, False, False, True, True, False, False,...   \n3  [True, True, True, False, False, True, False, ...   \n4  [False, False, False, False, False, False, Fal...   \n\n                                              labels  \n0  [O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...  \n1  [B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O...  \n2  [O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O...  \n3  [O, O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT...  \n4  [O, O, O, O, O, O, O, O, O, O, O, O, B-NAME_ST...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>document</th>\n      <th>full_text</th>\n      <th>tokens</th>\n      <th>trailing_whitespace</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7</td>\n      <td>Design Thinking for innovation reflexion-Avril...</td>\n      <td>[Design, Thinking, for, innovation, reflexion,...</td>\n      <td>[True, True, True, True, False, False, True, F...</td>\n      <td>[O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10</td>\n      <td>Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...</td>\n      <td>[Diego, Estrada, \\n\\n, Design, Thinking, Assig...</td>\n      <td>[True, False, False, True, True, False, False,...</td>\n      <td>[B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>16</td>\n      <td>Reporting process\\n\\nby Gilberto Gamboa\\n\\nCha...</td>\n      <td>[Reporting, process, \\n\\n, by, Gilberto, Gambo...</td>\n      <td>[True, False, False, True, True, False, False,...</td>\n      <td>[O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20</td>\n      <td>Design Thinking for Innovation\\n\\nSindy Samaca...</td>\n      <td>[Design, Thinking, for, Innovation, \\n\\n, Sind...</td>\n      <td>[True, True, True, False, False, True, False, ...</td>\n      <td>[O, O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>56</td>\n      <td>Assignment:  Visualization Reflection  Submitt...</td>\n      <td>[Assignment, :,   , Visualization,  , Reflecti...</td>\n      <td>[False, False, False, False, False, False, Fal...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, B-NAME_ST...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Part 2: Tokenization Function Definition","metadata":{}},{"cell_type":"code","source":"INFERENCE_MAX_LENGTH = 3700\n\ndef tokenize(example, tokenizer):\n    \n    text = []\n    token_map = []\n    \n    # Starting index at 0 to track tokens.\n    idx = 0\n    \n    # Iterating through tokens and their associated trailing whitespaces.\n    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n        \n        # Adding each token to the 'text' list.\n        text.append(t)\n        \n        # Extending 'token_map' with the current index repeated as many times as the length of the token.\n        token_map.extend([idx] * len(t))\n        \n        # Adding a space for trailing whitespace and marking it with '-1' in 'token_map'.\n        if ws:\n            text.append(\" \")\n            token_map.append(-1)\n            \n        # Incrementing 'idx' for the next token.\n        idx += 1\n        \n    # Tokenizing the concatenated 'text' and returning offset mappings along with 'token_map'.\n    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=False, max_length=INFERENCE_MAX_LENGTH)\n    \n    # Returning a dictionary containing the tokenized data and the 'token_map'.\n    return {\n        **tokenized,\n        \"token_map\": token_map,\n    }","metadata":{"execution":{"iopub.status.busy":"2024-04-20T12:28:36.020282Z","iopub.execute_input":"2024-04-20T12:28:36.020690Z","iopub.status.idle":"2024-04-20T12:28:36.028875Z","shell.execute_reply.started":"2024-04-20T12:28:36.020643Z","shell.execute_reply":"2024-04-20T12:28:36.027814Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Part 3: Loading Test Data and Tokenization","metadata":{}},{"cell_type":"code","source":"# Loading test data from JSON file\ntest_data = json.load(open(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\"))\n\n# Create a dataset from the loaded data\nds = Dataset.from_dict({\n    \"full_text\": [x[\"full_text\"] for x in test_data],\n    \"document\": [x[\"document\"] for x in test_data],\n    \"tokens\": [x[\"tokens\"] for x in test_data],\n    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in test_data],\n})\n\n# Initialize a tokenizer and model from the pretrained model path\n# model_paths = {'/kaggle/input/pii-deberta-models/cola-de-piiranha' : 1,\n#               '/kaggle/input/pii-deberta-models/cuerpo-de-piiranha' : 4, #3\n#               '/kaggle/input/pii-deberta-models/cabeza-de-piiranha' : 4, #4\n#               '/kaggle/input/pii-deberta-models/cabeza-del-piinguuino' : 6, #6\n#                '/kaggle/input/pii-deberta-models/deberta-v3-base-finetuned' : 6\n#               }\n\nmodel_paths = {\n    '/kaggle/input/37vp4pjt': 10/10,\n    '/kaggle/input/pii-deberta-models/cuerpo-de-piiranha': 2/10,\n    '/kaggle/input/pii-deberta-models/cola del piinguuino' : 1/10,\n    '/kaggle/input/pii-deberta-models/cabeza-del-piinguuino': 5/10,\n#     '/kaggle/input/pii-deberta-models/cabeza-de-piiranha': 3/10,\n#     '/kaggle/input/pii-deberta-models/cola-de-piiranha':1/10,\n    '/kaggle/input/pii-models/piidd-org-sakura': 8/10,\n#     '/kaggle/input/pii-deberta-models/cabeza-de-piiranha-persuade_v0':1/10,\n    }\n\nfirst_model_path = list(model_paths.keys())[0]\n\ntokenizer = AutoTokenizer.from_pretrained(first_model_path)\n\n# Tokenize the dataset using the 'tokenize' function in parallel\nds = ds.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer}, num_proc = 2)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T12:28:36.030894Z","iopub.execute_input":"2024-04-20T12:28:36.031203Z","iopub.status.idle":"2024-04-20T12:28:37.893553Z","shell.execute_reply.started":"2024-04-20T12:28:36.031169Z","shell.execute_reply":"2024-04-20T12:28:37.892597Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a490465b8134fc688efc05c96548884"}},"metadata":{}}]},{"cell_type":"code","source":"weights = list(model_paths.values())\nweights","metadata":{"execution":{"iopub.status.busy":"2024-04-20T12:28:37.895041Z","iopub.execute_input":"2024-04-20T12:28:37.895358Z","iopub.status.idle":"2024-04-20T12:28:37.902921Z","shell.execute_reply.started":"2024-04-20T12:28:37.895330Z","shell.execute_reply":"2024-04-20T12:28:37.901936Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"[1.0, 0.2, 0.1, 0.5, 0.3, 0.1, 0.2, 0.1]"},"metadata":{}}]},{"cell_type":"markdown","source":"# Part 4: Model Training and Prediction","metadata":{}},{"cell_type":"code","source":"# Directory for saving intermediate predictions\nimport gc\nimport torch\nimport numpy as np\n\nfrom scipy.special import softmax\n\n\nall_preds = []\n\n# Calculate the total weight\ntotal_weight = sum(model_paths.values())\n\n\n# Directory for saving intermediate predictions\nintermediate_dir = './model_predictions'\nos.makedirs(intermediate_dir, exist_ok=True)\n\nfor idx, (model_path, weight) in enumerate(model_paths.items()):\n    \n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    \n    model = AutoModelForTokenClassification.from_pretrained(model_path)\n    \n    collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n    \n    args = TrainingArguments(\n        \".\",\n        per_device_eval_batch_size=1,\n        report_to=\"none\",\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=args,\n        data_collator=collator,\n        tokenizer=tokenizer,\n    )\n    \n    predictions = trainer.predict(ds).predictions\n    weighted_predictions = softmax(predictions, axis=-1) * weight\n    \n    # Save weighted_predictions to disk\n    np.save(os.path.join(intermediate_dir, f'weighted_preds_model_{idx}.npy'), weighted_predictions)\n    \n    # Clear memory\n    del model, trainer, tokenizer, predictions, weighted_predictions\n    torch.cuda.empty_cache()\n    gc.collect()\n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-20T12:28:37.904136Z","iopub.execute_input":"2024-04-20T12:28:37.904410Z","iopub.status.idle":"2024-04-20T12:30:40.538948Z","shell.execute_reply.started":"2024-04-20T12:28:37.904390Z","shell.execute_reply":"2024-04-20T12:30:40.538123Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"markdown","source":"# Part 5: Aggregating Predictions","metadata":{}},{"cell_type":"code","source":"aggregated_predictions = None\n\nmodel_group = [[0,1,2], [3]]\n\nfor grp in model_group:\n    \n    preds = None\n    model_weights = 0\n    for idx in grp:\n        weighted_predictions = np.load(os.path.join(intermediate_dir, f'weighted_preds_model_{idx}.npy'))\n        preds = weighted_predictions if preds is None else preds + weighted_predictions  \n        model_weights += weights[idx]\n    preds /= model_weights\n    \n    aggregated_predictions = preds if aggregated_predictions is None else preds + aggregated_predictions\n    \nweighted_average_predictions = aggregated_predictions / len(model_group)\n\nconfig = json.load(open(Path(model_path) / \"config.json\"))\nid2label = config[\"id2label\"]\n\npreds = weighted_average_predictions.argmax(-1)\n\npreds_without_O = weighted_average_predictions[:,:,:12].argmax(-1)\n\nO_preds = weighted_average_predictions[:,:,12] # 12th column\n\nthreshold = 0.875\n\npreds_final = np.where(O_preds < threshold, preds_without_O , preds)\n\ntriplets = []\npairs = set()  # membership operation using set is faster O(1) than that of list O(n)\n\nprocessed = []\n\n# For each prediction, token mapping, offsets, tokens, and document in the dataset\nfor p, token_map, offsets, tokens, doc in zip(preds_final, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"]):\n\n    # Iterate through each token prediction and its corresponding offsets\n    for token_pred, (start_idx, end_idx) in zip(p, offsets):\n        label_pred = id2label[str(token_pred)]  # Predicted label from token\n\n        # If start and end indices sum to zero, continue to the next iteration\n        if start_idx + end_idx == 0:\n            continue\n\n        # If the token mapping at the start index is -1, increment start index\n        if token_map[start_idx] == -1:\n            start_idx += 1\n\n        # Ignore leading whitespace tokens (\"\\n\\n\")\n        while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n            start_idx += 1\n\n        # If start index exceeds the length of token mapping, break the loop\n        if start_idx >= len(token_map):\n            break\n\n        token_id = token_map[start_idx]  # Token ID at start index\n        \n        \"\"\"\n            {'B-EMAIL',\n             'B-ID_NUM',\n             'B-NAME_STUDENT',\n             'B-PHONE_NUM',\n             'B-STREET_ADDRESS',\n             'B-URL_PERSONAL',\n             'B-USERNAME',\n             'I-ID_NUM',\n             'I-NAME_STUDENT',\n             'I-PHONE_NUM',\n             'I-STREET_ADDRESS',\n             'I-URL_PERSONAL',\n             'O'}\n        \"\"\"\n\n        # Ignore \"O\" predictions and whitespace tokens\n        if label_pred in (\"O\", \"B-EMAIL\", \"B-PHONE_NUM\", \"I-PHONE_NUM\") or token_id == -1:\n            continue\n\n        pair = (doc, token_id)\n\n        if pair not in pairs:\n            processed.append({\"document\": doc, \"token\": token_id, \"label\": label_pred, \"token_str\": tokens[token_id]})\n            pairs.add(pair)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T12:30:40.540438Z","iopub.execute_input":"2024-04-20T12:30:40.540825Z","iopub.status.idle":"2024-04-20T12:30:40.647469Z","shell.execute_reply.started":"2024-04-20T12:30:40.540791Z","shell.execute_reply":"2024-04-20T12:30:40.646785Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Part 6: Post-processing and Saving Predictions","metadata":{}},{"cell_type":"code","source":"from spacy.lang.en import English\nnlp = English()\n\ndef find_span(target: list[str], document: list[str]) -> list[list[int]]:\n    idx = 0\n    spans = []\n    span = []\n\n    for i, token in enumerate(document):\n        if token != target[idx]:\n            idx = 0\n            span = []\n            continue\n        span.append(i)\n        idx += 1\n        if idx == len(target):\n            spans.append(span)\n            span = []\n            idx = 0\n            continue\n    \n    return spans\ndata = json.load(open(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\"))\n\nemail_regex = re.compile(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+')\nphone_num_regex = re.compile(r\"(\\(\\d{3}\\)\\d{3}\\-\\d{4}\\w*|\\d{3}\\.\\d{3}\\.\\d{4})\\s\")\nemails = []\nphone_nums = []\n\nfor _data in data:\n    # email\n    for token_idx, token in enumerate(_data[\"tokens\"]):\n        if re.fullmatch(email_regex, token) is not None:\n            emails.append(\n                {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": \"B-EMAIL\", \"token_str\": token}\n            )\n    # phone number\n    matches = phone_num_regex.findall(_data[\"full_text\"])\n    if not matches:\n        continue\n    for match in matches:\n        target = [t.text for t in nlp.tokenizer(match)]\n        matched_spans = find_span(target, _data[\"tokens\"])\n        \n    for matched_span in matched_spans:\n        for intermediate, token_idx in enumerate(matched_span):\n            prefix = \"I\" if intermediate else \"B\"\n            phone_nums.append(\n                {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": f\"{prefix}-PHONE_NUM\",\n                 \"token_str\": _data[\"tokens\"][token_idx]}\n            )\ndf = pd.DataFrame(processed + phone_nums + emails)\n\n# Assign each row a unique 'row_id'\ndf[\"row_id\"] = list(range(len(df)))\n\n# Display a glimpse of the first 100 rows of your data\ndisplay(df.head(100))\n\n# Cast your findings into a CSV file for further exploration\ndf[[\"row_id\", \"document\", \"token\", \"label\"]].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T12:30:40.648453Z","iopub.execute_input":"2024-04-20T12:30:40.648738Z","iopub.status.idle":"2024-04-20T12:30:43.521527Z","shell.execute_reply.started":"2024-04-20T12:30:40.648715Z","shell.execute_reply":"2024-04-20T12:30:43.520639Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"    document  token           label   token_str  row_id\n0          7      9  B-NAME_STUDENT    Nathalie       0\n1          7     10  I-NAME_STUDENT       Sylla       1\n2          7    482  B-NAME_STUDENT    Nathalie       2\n3          7    483  I-NAME_STUDENT       Sylla       3\n4          7    741  B-NAME_STUDENT    Nathalie       4\n5          7    742  I-NAME_STUDENT       Sylla       5\n6         10      0  B-NAME_STUDENT       Diego       6\n7         10      1  I-NAME_STUDENT     Estrada       7\n8         10    464  B-NAME_STUDENT       Diego       8\n9         10    465  I-NAME_STUDENT     Estrada       9\n10        16      4  B-NAME_STUDENT    Gilberto      10\n11        16      5  I-NAME_STUDENT      Gamboa      11\n12        20      5  B-NAME_STUDENT       Sindy      12\n13        20      6  I-NAME_STUDENT      Samaca      13\n14        56     12  B-NAME_STUDENT      Nadine      14\n15        56     13  I-NAME_STUDENT        Born      15\n16        86      6  B-NAME_STUDENT      Eladio      16\n17        86      7  I-NAME_STUDENT       Amaya      17\n18        93      0  B-NAME_STUDENT      Silvia      18\n19        93      1  I-NAME_STUDENT  Villalobos      19\n20       104      7  B-NAME_STUDENT          Dr      20\n21       104      8  B-NAME_STUDENT       Sakir      21\n22       104      9  I-NAME_STUDENT       Ahmad      22\n23       112      5  B-NAME_STUDENT   Francisco      23\n24       112      6  I-NAME_STUDENT    Ferreira      24\n25       123     32  B-NAME_STUDENT     Stefano      25\n26       123     33  I-NAME_STUDENT      Lovato      26","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>document</th>\n      <th>token</th>\n      <th>label</th>\n      <th>token_str</th>\n      <th>row_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7</td>\n      <td>9</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Nathalie</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>10</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Sylla</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7</td>\n      <td>482</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Nathalie</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7</td>\n      <td>483</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Sylla</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>741</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Nathalie</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>7</td>\n      <td>742</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Sylla</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>10</td>\n      <td>0</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Diego</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>10</td>\n      <td>1</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Estrada</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>10</td>\n      <td>464</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Diego</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>465</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Estrada</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>16</td>\n      <td>4</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Gilberto</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>16</td>\n      <td>5</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Gamboa</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>20</td>\n      <td>5</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Sindy</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>20</td>\n      <td>6</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Samaca</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>56</td>\n      <td>12</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Nadine</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>56</td>\n      <td>13</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Born</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>86</td>\n      <td>6</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Eladio</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>86</td>\n      <td>7</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Amaya</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>93</td>\n      <td>0</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Silvia</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>93</td>\n      <td>1</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Villalobos</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>104</td>\n      <td>7</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Dr</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>104</td>\n      <td>8</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Sakir</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>104</td>\n      <td>9</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Ahmad</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>112</td>\n      <td>5</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Francisco</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>112</td>\n      <td>6</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Ferreira</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>123</td>\n      <td>32</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Stefano</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>123</td>\n      <td>33</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Lovato</td>\n      <td>26</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}